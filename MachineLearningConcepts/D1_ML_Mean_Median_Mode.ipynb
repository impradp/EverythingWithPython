{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKqT92dWE0AU"
      },
      "source": [
        "# **Machine Learning(Getting Started)**\n",
        "\n",
        "## **Machine Learning**\n",
        "\n",
        "  - Machine Learning is making the computer learn from studying data and statistics.\n",
        "  - Machine Learning is a step into the direction of artificial intelligence (AI).\n",
        "  - Machine Learning is a program that analyses data and learns to predict the outcome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tks13uzGFSvS"
      },
      "source": [
        "## **Data Sets**\n",
        "In the mind of a computer, a data set is any collection of data. It can be anything from an array to a complete database.\n",
        "\n",
        "Example of an array:\n",
        "\n",
        "    [99,86,87,88,111,86,103,87,94,78,77,85,86]\n",
        "\n",
        "Example of a database:\n",
        "  \n",
        "    Here's a simple database table using markdown:\n",
        "\n",
        "| ID | First Name | Last Name | Email | Age | Program |\n",
        "|----|------------|-----------|-------|-----|---------|\n",
        "| 1 | John | Smith | john.smith@email.com | 32 | Masters |\n",
        "| 2 | Sarah | Johnson | sarah.j@email.com | 28 | Masters  |\n",
        "| 3 | Pradip | Puri | impradp@email.com | 45 | PhD |\n",
        "| 4 | Emma | Brown | emma.b@email.com | 35 | PhD |\n",
        "| 5 | Michael | Williams | mike.w@email.com | 22 | Bachelors |\n",
        "\n",
        "\n",
        "By looking at the array, we can guess that the average value is probably around 80 or 90, and we are also able to determine the highest value and the lowest value, but what else can we do?\n",
        "\n",
        "And by looking at the database we can see that the most popular program is Masters, and the youngest student to take program is 22 years, but what if we could predict if a student had a PhD program, just by looking at the other values?\n",
        "\n",
        "That is what Machine Learning is for! Analyzing data and predicting the outcome!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8JSaq6JHYsH"
      },
      "source": [
        "## **Data Types**\n",
        "\n",
        "Let me explain the key data types commonly encountered in machine learning.\n",
        "\n",
        "Numerical Data:\n",
        "- Continuous: Values that can take any number within a range (e.g., height, weight, temperature, prices)\n",
        "- Discrete: Count values or whole numbers (e.g., number of rooms, count of items)\n",
        "\n",
        "Categorical Data:\n",
        "- Nominal: Categories with no inherent order (e.g., colors, product names, cities)\n",
        "- Ordinal: Categories with a meaningful order (e.g., education level, customer satisfaction ratings)\n",
        "- Binary: Data with only two possible values (e.g., yes/no, true/false)\n",
        "\n",
        "Time Series Data:\n",
        "Data points collected over time at regular intervals (e.g., stock prices, weather measurements, sales figures)\n",
        "\n",
        "Text Data:\n",
        "Unstructured text that requires natural language processing (e.g., customer reviews, social media posts, articles)\n",
        "\n",
        "Image Data:\n",
        "Digital images represented as matrices of pixel values, commonly used in computer vision tasks\n",
        "\n",
        "Audio Data:\n",
        "Sound recordings represented as waveforms or spectrograms\n",
        "\n",
        "Video Data:\n",
        "Sequences of images with temporal information\n",
        "\n",
        "Each data type requires specific preprocessing techniques:\n",
        "- Numerical data often needs scaling or normalization\n",
        "- Categorical data typically requires encoding (one-hot, label, or ordinal encoding)\n",
        "- Text data needs tokenization, vectorization, or embedding\n",
        "- Image and audio data usually require normalization and may need resizing or feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2N730URHoAB"
      },
      "source": [
        "## **Mean**\n",
        "The average value from the given datasets.\n",
        "\n",
        "To calculate the mean, find the sum of all values, and divide the sum by the number of values.\n",
        "\n",
        "    (99+86+87+88+111+86+103+87+94+78+77+85+86) / 13 = 89.77\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxIDJEi8H1v0"
      },
      "outputs": [],
      "source": [
        "# Calculating the mean using numpy library/module\n",
        "import numpy\n",
        "\n",
        "speed = [99,86,87,88,111,86,103,87,94,78,77,85,86]\n",
        "\n",
        "x = numpy.mean(speed)\n",
        "\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToAVgK9RIGHC"
      },
      "source": [
        "## **Median**\n",
        "The median value is the value in the middle, after you have sorted all the values.\n",
        "                            \n",
        "    77, 78, 85, 86, 86, 86, 87, 87, 88, 94, 99, 103, 111"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMAI2a58IYBl",
        "outputId": "40875f80-6498-47f2-a8ab-25de63819c70"
      },
      "outputs": [],
      "source": [
        "# Calculating median\n",
        "import numpy\n",
        "\n",
        "speed = [99,86,87,88,111,86,103,87,94,78,77,85,86]\n",
        "\n",
        "x = numpy.median(speed)\n",
        "\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KG6OrXBeIhym"
      },
      "source": [
        "## **Mode**\n",
        "The Mode value is the value that appears the most number of times.\n",
        "\n",
        "      99, 86, 87, 88, 111, 86, 103, 87, 94, 78, 77, 85, 86 = 86\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX5rpjspIuD6"
      },
      "outputs": [],
      "source": [
        "# Calculating mode from scipy\n",
        "from scipy import stats\n",
        "\n",
        "speed = [99,86,87,88,111,86,103,87,94,78,77,85,86]\n",
        "\n",
        "x = stats.mode(speed)\n",
        "\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_rycM8zM2Cb"
      },
      "source": [
        "## **Standard Deviation**\n",
        "\n",
        "Standard deviation is a statistical measure that quantifies the amount of variation or dispersion in a dataset.\n",
        "\n",
        "In simpler terms, it tells you how spread out your numbers are from their average (mean) value.\n",
        "\n",
        "- A low standard deviation means the values tend to be close to the mean\n",
        "- A high standard deviation means the values are spread out over a wider range\n",
        "\n",
        "\n",
        "Some important notes about NumPy's standard deviation functions:\n",
        "\n",
        "- *np.std()* by default uses *ddof=0* (population standard deviation)\n",
        "- Use *ddof=1* when working with a sample of a larger population (sample standard deviation)\n",
        "- NumPy has both *np.std()* and *array.std()* methods that work the same way\n",
        "\n",
        "Mathematical steps:\n",
        "\n",
        "    # Manual calculation example\n",
        "    differences = scores - mean\n",
        "    squared_diff = differences ** 2\n",
        "    variance = np.mean(squared_diff)\n",
        "    manual_std = np.sqrt(variance)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8J-blw2jOBjV",
        "outputId": "21fdc257-4355-463a-ec8f-d4ac498daacb"
      },
      "outputs": [],
      "source": [
        "# Standard Deviation Calculation\n",
        "import numpy as np\n",
        "\n",
        "# Sample dataset: test scores of students\n",
        "scores = np.array([85, 90, 72, 95, 88, 82, 78, 85, 92, 89])\n",
        "\n",
        "# Calculate mean\n",
        "mean = np.mean(scores)\n",
        "print(f\"Mean score: {mean}\")\n",
        "\n",
        "# Calculate standard deviation\n",
        "std_dev = np.std(scores)\n",
        "print(f\"Standard deviation: {std_dev}\")\n",
        "\n",
        "# You can also specify ddof (delta degrees of freedom)\n",
        "# ddof=1 gives sample standard deviation (commonly used in statistics)\n",
        "sample_std = np.std(scores, ddof=1)\n",
        "print(f\"Sample standard deviation: {sample_std}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVKbXU6pPReq"
      },
      "source": [
        "## **Variance**\n",
        "Variance is a measure of spread that shows how far a set of numbers are from their mean. It's actually the square of standard deviation. In other words, it's the average of squared differences from the mean.\n",
        "\n",
        "Key Takeways:\n",
        "\n",
        "- Variance is always positive (because we're squaring differences)\n",
        "- Larger variance indicates more spread out data\n",
        "- Units are squared (if your data is in meters, variance is in square meters)\n",
        "- Like standard deviation, you can calculate population variance (ddof=0) or sample variance (ddof=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8B2kP_ePYDK"
      },
      "outputs": [],
      "source": [
        "# Variance Calculation\n",
        "import numpy as np\n",
        "\n",
        "# Sample dataset\n",
        "data = np.array([4, 8, 6, 2, 9, 5, 7, 3])\n",
        "\n",
        "# Calculate variance using NumPy\n",
        "variance = np.var(data)\n",
        "print(f\"Variance using np.var(): {variance}\")\n",
        "\n",
        "# Calculate sample variance (n-1 denominator)\n",
        "sample_variance = np.var(data, ddof=1)\n",
        "print(f\"Sample variance: {sample_variance}\")\n",
        "\n",
        "# Let's break down how variance is calculated manually:\n",
        "# 1. Calculate the mean\n",
        "mean = np.mean(data)\n",
        "\n",
        "# 2. Calculate squared differences from mean\n",
        "squared_diff = (data - mean) ** 2\n",
        "\n",
        "# 3. Calculate variance (mean of squared differences)\n",
        "manual_variance = np.mean(squared_diff)\n",
        "print(f\"Manual variance calculation: {manual_variance}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aC-Ow1BPwZA"
      },
      "outputs": [],
      "source": [
        "# Optional\n",
        "# Variance along specific axis of multi-dimensional array\n",
        "array_2d = np.array([[1, 2, 3],\n",
        "                     [4, 5, 6],\n",
        "                     [7, 8, 9]])\n",
        "\n",
        "# Variance of each column\n",
        "col_variance = np.var(array_2d, axis=0)\n",
        "print(f\"Column variances: {col_variance}\")\n",
        "\n",
        "# Variance of each row\n",
        "row_variance = np.var(array_2d, axis=1)\n",
        "print(f\"Row variances: {row_variance}\")\n",
        "\n",
        "# Calculate covariance between two variables\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([2, 4, 5, 4, 5])\n",
        "covariance = np.cov(x, y)[0][1]  # Extract covariance from 2x2 matrix\n",
        "print(f\"Covariance between x and y: {covariance}\")\n",
        "\n",
        "# Standard deviation is square root of variance\n",
        "std_dev = np.sqrt(variance)\n",
        "print(f\"Standard deviation: {std_dev}\")\n",
        "print(f\"Variance: {variance}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twR_EhLcQHxB"
      },
      "source": [
        "## **Percentiles**\n",
        "\n",
        "A percentile indicates the value below which a given percentage of observations falls.\n",
        "\n",
        "For example, if you score in the 90th percentile, you performed better than 90% of the group.\n",
        "\n",
        "  1.  25th percentile (Q1): First quartile\n",
        "  2.  50th percentile (Q2): Median\n",
        "  3.  75th percentile (Q3): Third quartile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kb9ZU_d2Qv2S"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sample dataset - let's say these are test scores\n",
        "scores = np.array([55, 62, 68, 74, 77, 82, 84, 85, 87, 91, 93, 95, 98])\n",
        "\n",
        "# Calculate various percentiles\n",
        "median = np.percentile(scores, 50)  # 50th percentile is the median\n",
        "quartile_25 = np.percentile(scores, 25)  # First quartile\n",
        "quartile_75 = np.percentile(scores, 75)  # Third quartile\n",
        "percentile_90 = np.percentile(scores, 90)  # 90th percentile\n",
        "\n",
        "print(f\"Median (50th percentile): {median}\")\n",
        "print(f\"25th percentile: {quartile_25}\")\n",
        "print(f\"75th percentile: {quartile_75}\")\n",
        "print(f\"90th percentile: {percentile_90}\")\n",
        "\n",
        "# Calculate multiple percentiles at once\n",
        "percentiles = np.percentile(scores, [25, 50, 75, 90])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIvcfqN1RacC"
      },
      "source": [
        "### **Interpolation Methods in Percentile**\n",
        "NumPy offers different interpolation methods through the method parameter.\n",
        "\n",
        "Common Applications:\n",
        "\n",
        "  - Performance Assessment (test scores, athletic performance)\n",
        "  - Income Distribution Analysis\n",
        "  - Growth Charts (height, weight percentiles)\n",
        "  - Outlier Detection using IQR method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrDN2ew0RVq8"
      },
      "outputs": [],
      "source": [
        "# Different interpolation methods\n",
        "linear = np.percentile(scores, 75, method='linear')  # Default\n",
        "nearest = np.percentile(scores, 75, method='nearest')\n",
        "lower = np.percentile(scores, 75, method='lower')\n",
        "higher = np.percentile(scores, 75, method='higher')\n",
        "midpoint = np.percentile(scores, 75, method='midpoint')\n",
        "\n",
        "# Real-world example: Calculate percentile rank of a score\n",
        "def percentile_rank(data, score):\n",
        "    \"\"\"Calculate the percentile rank of a score within a dataset\"\"\"\n",
        "    return len(data[data <= score]) / len(data) * 100\n",
        "\n",
        "score_to_check = 85\n",
        "rank = percentile_rank(scores, score_to_check)\n",
        "print(f\"A score of {score_to_check} is in the {rank:.1f}th percentile\")\n",
        "\n",
        "# Interquartile Range (IQR) - common measure of spread\n",
        "iqr = np.percentile(scores, 75) - np.percentile(scores, 25)\n",
        "print(f\"Interquartile Range: {iqr}\")\n",
        "\n",
        "# Using nanpercentile for data with missing values\n",
        "data_with_nan = np.array([1, 2, np.nan, 4, 5])\n",
        "percentile_with_nan = np.nanpercentile(data_with_nan, 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zf5h2LEMR_12",
        "outputId": "5b3637ed-2855-45d2-ad0a-7bc7968e9d05"
      },
      "outputs": [],
      "source": [
        "### **Outlier Detection using Percentiles**\n",
        "\n",
        "# Detect outliers using IQR method\n",
        "q1 = np.percentile(scores, 25)\n",
        "q3 = np.percentile(scores, 75)\n",
        "iqr = q3 - q1\n",
        "lower_bound = q1 - 1.5 * iqr\n",
        "upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "outliers = scores[(scores < lower_bound) | (scores > upper_bound)]\n",
        "print(f\"Outliers: {outliers}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
